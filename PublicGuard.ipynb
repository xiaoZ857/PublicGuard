{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589e4bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PublicGuard: Audi Alteram Partem (AAP) Framework Implementation\n",
    "Real-time hallucination detection for critical public services\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from LLMAPI import LLMAPI\n",
    "\n",
    "# Configure logging for audit trail\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('publicguard_audit.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('PublicGuard')\n",
    "\n",
    "class PublicGuard:\n",
    "    \"\"\"\n",
    "    Main class for the Audi Alteram Partem hallucination detection framework.\n",
    "    Aggregates predictions from multiple LLMs using Bayesian inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = \"config.json\"):\n",
    "        \"\"\"\n",
    "        Initialize PublicGuard with model configurations.\n",
    "        \n",
    "        Args:\n",
    "            config_path: Path to configuration file containing model priors\n",
    "        \"\"\"\n",
    "        # Default model priors computed from MCC scores: P = (MCC + 1) / 2\n",
    "        self.DEFAULT_PRIORS = {\n",
    "            \"phi4\": 0.734,\n",
    "            \"qwen2.5\": 0.724,\n",
    "            \"mistral\": 0.642,\n",
    "            \"gemma3\": 0.633,\n",
    "            \"llama3.2\": 0.573,\n",
    "            \"deepseek-r1\": 0.710,\n",
    "            \"moonshot-v1-8k\": 0.689,\n",
    "        }\n",
    "        \n",
    "        # Load custom configuration if available\n",
    "        self.model_priors = self._load_config(config_path)\n",
    "        \n",
    "        # Standard prompt template for consistent model responses\n",
    "        self.PROMPT_TEMPLATE = \"\"\"Determine if the following statement is TRUE or FALSE.\n",
    "Only answer with \"TRUE\" or \"FALSE\".\n",
    "\n",
    "Statement: \"{statement}\"\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Audit trail storage\n",
    "        self.audit_trail = []\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Load model priors from configuration file.\n",
    "        \n",
    "        Args:\n",
    "            config_path: Path to JSON configuration file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of model priors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                logger.info(f\"Loaded configuration from {config_path}\")\n",
    "                return config.get('model_priors', self.DEFAULT_PRIORS)\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Config file {config_path} not found, using defaults\")\n",
    "            return self.DEFAULT_PRIORS\n",
    "    \n",
    "    def _query_model(self, model_name: str, statement: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Query a single model for hallucination detection.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to query\n",
    "            statement: Statement to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            \"TRUE\" or \"FALSE\" prediction, or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format the prompt\n",
    "            prompt = self.PROMPT_TEMPLATE.format(statement=statement)\n",
    "            \n",
    "            # Call the model API\n",
    "            response = LLMAPI(prompt, model_name)\n",
    "            \n",
    "            # Extract TRUE/FALSE from response using pattern matching\n",
    "            if re.search(r\"\\bTRUE\\b\", response, re.IGNORECASE):\n",
    "                return \"TRUE\"\n",
    "            elif re.search(r\"\\bFALSE\\b\", response, re.IGNORECASE):\n",
    "                return \"FALSE\"\n",
    "            else:\n",
    "                # Default to FALSE for ambiguous responses\n",
    "                logger.warning(f\"Ambiguous response from {model_name}: {response}\")\n",
    "                return \"FALSE\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error querying {model_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _compute_posterior(self, predictions: Dict[str, str]) -> float:\n",
    "        \"\"\"\n",
    "        Compute posterior probability using Bayesian aggregation.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Dictionary of {model_name: prediction}\n",
    "            \n",
    "        Returns:\n",
    "            Posterior probability that the statement is TRUE\n",
    "        \"\"\"\n",
    "        # Initialize likelihood ratios\n",
    "        likelihood_true = 1.0\n",
    "        likelihood_false = 1.0\n",
    "        \n",
    "        # Update likelihoods based on each model's prediction and prior\n",
    "        for model, prediction in predictions.items():\n",
    "            prior = self.model_priors.get(model, 0.5)  # Default to 0.5 if unknown\n",
    "            \n",
    "            if prediction == \"TRUE\":\n",
    "                # Model says TRUE\n",
    "                likelihood_true *= prior\n",
    "                likelihood_false *= (1 - prior)\n",
    "            else:\n",
    "                # Model says FALSE\n",
    "                likelihood_true *= (1 - prior)\n",
    "                likelihood_false *= prior\n",
    "        \n",
    "        # Compute posterior probability using Bayes' rule\n",
    "        total_likelihood = likelihood_true + likelihood_false\n",
    "        \n",
    "        if total_likelihood > 0:\n",
    "            posterior = likelihood_true / total_likelihood\n",
    "        else:\n",
    "            # Handle edge case of zero likelihood\n",
    "            posterior = 0.5\n",
    "            \n",
    "        return posterior\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 statement: str, \n",
    "                 num_models: int = 5,\n",
    "                 confidence_threshold: float = 0.5,\n",
    "                 context: str = \"general\") -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a statement for potential hallucination using multiple models.\n",
    "        \n",
    "        Args:\n",
    "            statement: The statement to evaluate\n",
    "            num_models: Number of models to use (top performers)\n",
    "            confidence_threshold: Threshold for TRUE/FALSE decision\n",
    "            context: Application context (e.g., \"medical\", \"legal\", \"emergency\")\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - is_truthful: Boolean indicating if statement is truthful\n",
    "                - confidence: Float confidence score (0-1)\n",
    "                - prediction: \"TRUE\" or \"FALSE\"\n",
    "                - model_votes: Individual model predictions\n",
    "                - audit_id: Unique identifier for audit trail\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        audit_id = f\"AAP-{start_time.strftime('%Y%m%d-%H%M%S-%f')}\"\n",
    "        \n",
    "        logger.info(f\"Starting evaluation {audit_id} for context: {context}\")\n",
    "        \n",
    "        # Select top models based on prior probabilities\n",
    "        sorted_models = sorted(self.model_priors.items(), \n",
    "                             key=lambda x: x[1], \n",
    "                             reverse=True)\n",
    "        selected_models = [model for model, _ in sorted_models[:num_models]]\n",
    "        \n",
    "        # Query each selected model\n",
    "        predictions = {}\n",
    "        for model in selected_models:\n",
    "            prediction = self._query_model(model, statement)\n",
    "            if prediction is not None:\n",
    "                predictions[model] = prediction\n",
    "                logger.info(f\"Model {model} predicted: {prediction}\")\n",
    "        \n",
    "        # Ensure we have enough predictions\n",
    "        if len(predictions) < 2:\n",
    "            logger.error(\"Insufficient model responses for reliable evaluation\")\n",
    "            return {\n",
    "                \"error\": \"Insufficient model responses\",\n",
    "                \"audit_id\": audit_id\n",
    "            }\n",
    "        \n",
    "        # Compute posterior probability\n",
    "        posterior = self._compute_posterior(predictions)\n",
    "        \n",
    "        # Make final decision\n",
    "        final_prediction = \"TRUE\" if posterior >= confidence_threshold else \"FALSE\"\n",
    "        \n",
    "        # Adjust confidence score for interpretability\n",
    "        # If predicting FALSE, confidence is 1 - posterior\n",
    "        if final_prediction == \"FALSE\":\n",
    "            confidence = 1 - posterior\n",
    "        else:\n",
    "            confidence = posterior\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"is_truthful\": final_prediction == \"TRUE\",\n",
    "            \"confidence\": round(confidence, 3),\n",
    "            \"prediction\": final_prediction,\n",
    "            \"raw_posterior\": round(posterior, 3),\n",
    "            \"model_votes\": predictions,\n",
    "            \"num_models_used\": len(predictions),\n",
    "            \"audit_id\": audit_id,\n",
    "            \"context\": context,\n",
    "            \"evaluation_time_ms\": int((datetime.now() - start_time).total_seconds() * 1000)\n",
    "        }\n",
    "        \n",
    "        # Log to audit trail\n",
    "        audit_entry = {\n",
    "            **result,\n",
    "            \"statement\": statement[:200] + \"...\" if len(statement) > 200 else statement,\n",
    "            \"timestamp\": start_time.isoformat()\n",
    "        }\n",
    "        self.audit_trail.append(audit_entry)\n",
    "        \n",
    "        # Log summary\n",
    "        logger.info(f\"Evaluation complete: {final_prediction} \"\n",
    "                   f\"(confidence: {confidence:.3f}, time: {result['evaluation_time_ms']}ms)\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_evaluate(self, statements: List[str], **kwargs) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate multiple statements in batch.\n",
    "        \n",
    "        Args:\n",
    "            statements: List of statements to evaluate\n",
    "            **kwargs: Additional arguments passed to evaluate()\n",
    "            \n",
    "        Returns:\n",
    "            List of evaluation results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i, statement in enumerate(statements):\n",
    "            logger.info(f\"Processing batch item {i+1}/{len(statements)}\")\n",
    "            result = self.evaluate(statement, **kwargs)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_audit_trail(self, \n",
    "                       start_date: Optional[str] = None,\n",
    "                       end_date: Optional[str] = None,\n",
    "                       context: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve audit trail with optional filtering.\n",
    "        \n",
    "        Args:\n",
    "            start_date: Filter by start date (ISO format)\n",
    "            end_date: Filter by end date (ISO format)\n",
    "            context: Filter by application context\n",
    "            \n",
    "        Returns:\n",
    "            Filtered audit trail entries\n",
    "        \"\"\"\n",
    "        filtered_trail = self.audit_trail\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if start_date:\n",
    "            filtered_trail = [e for e in filtered_trail \n",
    "                            if e['timestamp'] >= start_date]\n",
    "        \n",
    "        if end_date:\n",
    "            filtered_trail = [e for e in filtered_trail \n",
    "                            if e['timestamp'] <= end_date]\n",
    "        \n",
    "        if context:\n",
    "            filtered_trail = [e for e in filtered_trail \n",
    "                            if e.get('context') == context]\n",
    "        \n",
    "        return filtered_trail\n",
    "    \n",
    "    def calibrate_model(self, \n",
    "                       model_name: str,\n",
    "                       test_statements: List[Tuple[str, bool]]) -> float:\n",
    "        \"\"\"\n",
    "        Calibrate a new model by computing its prior probability.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to calibrate\n",
    "            test_statements: List of (statement, is_true) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Computed prior probability for the model\n",
    "        \"\"\"\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for statement, ground_truth in test_statements:\n",
    "            prediction = self._query_model(model_name, statement)\n",
    "            if prediction is not None:\n",
    "                total_predictions += 1\n",
    "                is_correct = (prediction == \"TRUE\") == ground_truth\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "        \n",
    "        if total_predictions > 0:\n",
    "            # Compute MCC-based prior\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            # Simplified MCC approximation\n",
    "            mcc_approx = 2 * accuracy - 1\n",
    "            prior = (mcc_approx + 1) / 2\n",
    "            \n",
    "            logger.info(f\"Calibrated {model_name}: accuracy={accuracy:.3f}, prior={prior:.3f}\")\n",
    "            return prior\n",
    "        else:\n",
    "            logger.error(f\"Failed to calibrate {model_name}: no valid predictions\")\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize PublicGuard\n",
    "    guard = PublicGuard()\n",
    "    \n",
    "    # Example 1: Medical context\n",
    "    medical_statement = \"Aspirin is commonly used to reduce fever and relieve mild to moderate pain.\"\n",
    "    result = guard.evaluate(\n",
    "        medical_statement,\n",
    "        num_models=5,\n",
    "        confidence_threshold=0.5,\n",
    "        context=\"medical\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== PublicGuard Evaluation Result ===\")\n",
    "    print(f\"Statement: {medical_statement[:100]}...\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "    print(f\"Is Truthful: {result['is_truthful']}\")\n",
    "    print(f\"Audit ID: {result['audit_id']}\")\n",
    "    print(f\"Model Consensus: {result['model_votes']}\")\n",
    "    \n",
    "    # Example 2: Batch evaluation for legal context\n",
    "    legal_statements = [\n",
    "        \"A contract requires offer, acceptance, and consideration to be valid.\",\n",
    "        \"In the US, jury verdicts in civil cases must always be unanimous.\",\n",
    "        \"Attorney-client privilege protects all communications between lawyer and client.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Batch Evaluation for Legal Context ===\")\n",
    "    batch_results = guard.batch_evaluate(\n",
    "        legal_statements,\n",
    "        num_models=4,\n",
    "        context=\"legal\"\n",
    "    )\n",
    "    \n",
    "    for i, result in enumerate(batch_results):\n",
    "        print(f\"\\nStatement {i+1}: {result['prediction']} \"\n",
    "              f\"(confidence: {result['confidence']:.1%})\")\n",
    "    \n",
    "    # Example 3: Retrieve audit trail\n",
    "    print(\"\\n=== Audit Trail Summary ===\")\n",
    "    audit_entries = guard.get_audit_trail(context=\"medical\")\n",
    "    print(f\"Total medical evaluations: {len(audit_entries)}\")\n",
    "    \n",
    "    if audit_entries:\n",
    "        avg_confidence = sum(e['confidence'] for e in audit_entries) / len(audit_entries)\n",
    "        print(f\"Average confidence: {avg_confidence:.1%}\")\n",
    "        \n",
    "    # Example 4: High-stakes emergency context with higher threshold\n",
    "    emergency_statement = \"In case of cardiac arrest, begin CPR with 30 chest compressions.\"\n",
    "    emergency_result = guard.evaluate(\n",
    "        emergency_statement,\n",
    "        num_models=7,  # Use more models for critical decisions\n",
    "        confidence_threshold=0.7,  # Higher threshold for safety\n",
    "        context=\"emergency\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Emergency Response Evaluation ===\")\n",
    "    print(f\"Critical Statement Evaluation:\")\n",
    "    print(f\"Confidence Level: {emergency_result['confidence']:.1%}\")\n",
    "    print(f\"Recommendation: {'ACCEPT' if emergency_result['confidence'] > 0.9 else 'REQUIRE HUMAN REVIEW'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
